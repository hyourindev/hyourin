{
	"hello": {
		"en": {
			"title": "Hello World",
			"date": "2026-01-27",
			"content": "This is my first post. Welcome to hyourin.dev."
		},
		"ja": {
			"title": "はじめまして",
			"date": "2026-01-27",
			"content": "最初の投稿です。hyourin.devへようこそ。"
		}
	},
	"nomic-c": {
		"en": {
			"title": "nomic.c",
			"date": "2026-01-27",
			"content": "[github.com/hyourindev/nomic.c](https://github.com/hyourindev/nomic.c)\n\n## Pure C Inference for Nomic Embed Text v1.5\n\nI wrote a text embedding model from scratch in C. No PyTorch, no ONNX, no BLAS libraries. Just C11 and Intel intrinsics. The result is a single-file implementation of Nomic Embed Text v1.5 that runs 1.2 to 2.1x faster than HuggingFace Transformers on CPU.\n\nWhat follows is the reasoning behind every major decision, every optimization that worked, and a few that didn't.\n\n\n\n## Why Pure C\n\nEmbedding models are the backbone of semantic search, RAG pipelines, clustering, classification. Every time you type a query into a search bar backed by vectors, an embedding model runs somewhere. Usually that means PyTorch. Usually that means pulling in a 2GB runtime, a Python interpreter, and hoping your dependency graph doesn't break.\n\nI wanted something different. I wanted to call one function from any C program, get back a float array, and be done. No runtime. No allocator hidden behind three abstraction layers. No dynamic dispatch. Just `#include \"nomic.h\"` and link with `-lm`.\n\nNomic Embed v1.5 was the right target. It's a 137M parameter BERT encoder. Small enough that the weights fit in 522MB of float32. Big enough that naive code is slow and you actually have to think about performance. It also supports Matryoshka dimensions, which means you can truncate embeddings to 64, 128, 256, or 512 floats and still get useful results. That's a practical feature worth supporting.\n\n\n\n## The Model\n\nThe architecture is BERT, but not vanilla BERT. Nomic made several changes that affect the implementation.\n\nRotary Position Embeddings replace learned absolute position embeddings. Instead of adding a position vector to the token embeddings, RoPE rotates pairs of dimensions by an angle proportional to the position. The rotation frequencies use Dynamic NTK scaling, which adjusts the base frequency when the sequence length exceeds the training length. This means I need to compute sin/cos tables dynamically based on actual sequence length, not just look them up from a static table.\n\nThe feed-forward network uses SwiGLU instead of GELU. SwiGLU splits the intermediate representation into two halves, applies SiLU (sigmoid linear unit) to one half, and multiplies elementwise. This means the FFN projection goes from 768 to 3072 twice (gate and value), rather than once. More parameters, more FLOPs, but better quality per parameter.\n\nThe QKV projection is fused. Instead of three separate 768x768 linear layers for Q, K, and V, there's a single 768x2304 projection. No bias terms anywhere. This is a minor implementation detail but it simplifies the code.\n\nPost-norm residual connections instead of pre-norm. The layer norm comes after the residual add, not before the attention or FFN blocks. This changes the forward pass order slightly.\n\nThe final embedding pipeline is mean pooling over the sequence, followed by a final layer norm, truncation to the desired Matryoshka dimension, and L2 normalization. Every embedding that comes out of `nomic_embed` has unit norm, ready for cosine similarity via dot product.\n\n\n\n## The Tokenizer\n\nThe tokenizer was the hardest part of the project. Not because tokenization is conceptually difficult, but because BERT tokenization has accumulated years of edge cases that all have to match exactly.\n\nThe pipeline starts with Unicode normalization. Specifically NFD normalization, which decomposes accented characters into base character plus combining mark. Then accent stripping removes the combining marks entirely. So \"café\" becomes \"cafe\". This is important because the vocabulary was built this way, and if your tokenizer doesn't match, you get different token IDs, and different token IDs produce completely wrong embeddings.\n\nAfter normalization, CJK characters get spaces inserted around them. This is a BERT-specific rule that treats every CJK unified ideograph as its own word. Then everything is lowercased, and the text is split on whitespace and punctuation boundaries simultaneously.\n\nEach resulting word goes through WordPiece splitting. The algorithm tries to match the longest prefix against the vocabulary. If the whole word matches, great. If not, take the longest matching prefix, emit it, and continue with the remainder prefixed by `##`. If no prefix matches at all, emit `[UNK]`.\n\nThe vocabulary has 30522 entries. I store them as a sorted array and use binary search for lookups. I tried a hash table first, but the sorted array was actually faster for this vocabulary size because of cache locality. Each lookup hits a contiguous region of memory, while the hash table scattered accesses across the heap.\n\nGetting tokenization exactly right took more time than the entire SIMD implementation. I wrote over 20 tokenizer-specific tests, covering things like mixed CJK and Latin text, strings with multiple consecutive punctuation marks, words that decompose into many subword tokens, and the empty string. One wrong token and the embedding is garbage. There's no graceful degradation.\n\n\n\n## SIMD Strategy\n\nAll the compute-heavy operations in the model are either matrix multiplies or element-wise vector operations. Both map naturally to SIMD.\n\nI targeted AVX2 with FMA because it's available on every x86-64 CPU made in the last decade. Each `__m256` register holds 8 float32 values. Every dimension in the model (768, 2304, 3072) is divisible by 8, which means no tail handling. Every loop processes exactly 8 floats per iteration with no remainder and no masking.\n\nThe three utility functions that everything else builds on:\n\n`hsum_avx` reduces 8 floats in a `__m256` to a single scalar. It uses two `_mm256_hadd_ps` operations followed by extracting and adding the high and low 128-bit lanes. This is the most common operation in the codebase because every dot product ends with a horizontal sum.\n\n`exp256_approx` computes a vectorized approximation of `exp(x)` for 8 values simultaneously. The algorithm decomposes `exp(x)` into `2^(x/ln2)`, splits that into an integer part (which becomes a bit shift on the IEEE 754 exponent) and a fractional part (approximated by a degree-4 Horner polynomial). The result has about 20 bits of accuracy. For softmax and SiLU, where we're computing ratios of exponentials, the relative error cancels out and 20 bits is more than enough.\n\n`rcp_nr` computes `1/x` using `_mm256_rcp_ps` (which gives 12-bit accuracy) followed by one Newton-Raphson refinement step that doubles the precision. I use this in the SiLU sigmoid computation to avoid `_mm256_div_ps`, which is 3-4x slower than multiply on most microarchitectures.\n\nThe entire SIMD layer compiles away to nothing when `USE_AVX2` isn't defined. Every kernel has a scalar fallback that uses plain C loops. This means the code builds and runs correctly on ARM, RISC-V, or any other architecture. Just slower.\n\n\n\n## The GEMM Micro-Kernel\n\nThe single most important function in the entire codebase is `linear_no_bias`. It does the matrix multiply for every linear projection in every transformer layer. Q, K, V projections, output projection, FFN up-projection, FFN down-projection. Twelve layers, six multiplies each. If this function is slow, everything is slow.\n\nMy first implementation was the naive triple loop. It was correct and painfully slow.\n\nMy second implementation processed one output neuron at a time with an AVX2 dot product. Better, but still leaving performance on the table. The bottleneck was memory bandwidth. Each output neuron loads one row of the weight matrix, uses it once, and discards it. The arithmetic intensity is too low.\n\nThe third implementation is what shipped. It's a 2S x 4O micro-kernel that processes 2 sequence positions and 4 output neurons simultaneously. The inner loop loads 8 weights from each of the 4 output rows, broadcasts 8 input values from each of the 2 sequence rows, and performs 8 FMA operations into 8 accumulator registers.\n\n```c\n__m256 a00 = _mm256_setzero_ps(), a01 = a00, a02 = a00, a03 = a00;\n__m256 a10 = _mm256_setzero_ps(), a11 = a10, a12 = a10, a13 = a10;\n\nfor (int k = 0; k < in; k += 8) {\n    __m256 x0 = _mm256_load_ps(row0 + k);\n    __m256 x1 = _mm256_load_ps(row1 + k);\n    __m256 w0 = _mm256_load_ps(W0 + k);\n    __m256 w1 = _mm256_load_ps(W1 + k);\n    __m256 w2 = _mm256_load_ps(W2 + k);\n    __m256 w3 = _mm256_load_ps(W3 + k);\n\n    a00 = _mm256_fmadd_ps(x0, w0, a00);\n    a01 = _mm256_fmadd_ps(x0, w1, a01);\n    a02 = _mm256_fmadd_ps(x0, w2, a02);\n    a03 = _mm256_fmadd_ps(x0, w3, a03);\n    a10 = _mm256_fmadd_ps(x1, w0, a10);\n    a11 = _mm256_fmadd_ps(x1, w1, a11);\n    a12 = _mm256_fmadd_ps(x1, w2, a12);\n    a13 = _mm256_fmadd_ps(x1, w3, a13);\n}\n```\n\nThat's 8 accumulators, 2 input loads, and 4 weight loads per iteration. 14 YMM registers out of 16 available. The two spare registers are used by the FMA pipeline for temporaries. The register pressure is tight but it fits.\n\nThe key insight is that each weight load is shared across both sequence rows. This halves the weight bandwidth compared to processing one row at a time. The arithmetic intensity doubles, and the kernel becomes compute-bound rather than memory-bound.\n\nI also added software prefetching on the weight rows, 16 floats (64 bytes, one cache line) ahead of the current position:\n\n```c\n_mm_prefetch((const char *)(W0 + k + 16), _MM_HINT_T0);\n_mm_prefetch((const char *)(W1 + k + 16), _MM_HINT_T0);\n_mm_prefetch((const char *)(W2 + k + 16), _MM_HINT_T0);\n_mm_prefetch((const char *)(W3 + k + 16), _MM_HINT_T0);\n```\n\nThis tells the CPU to start fetching the next cache line before we need it. The benefit is modest on modern CPUs with good hardware prefetchers, but it's free to do and helps on older hardware.\n\n\n\n## Parallelism\n\nA single-threaded implementation is simple but leaves most of the CPU idle. The model has plenty of data parallelism to exploit. I used OpenMP because it's dead simple and GCC supports it natively.\n\nThe first place I added parallelism was the GEMM kernel. The naive approach is to parallelize over sequence positions, but that leaves cores idle when the sequence is short. A 10-token query produces a 12x768 matrix after adding CLS and SEP. Parallelizing over 6 pairs of rows across 6 threads gives each thread exactly one pair. No load balancing.\n\nI restructured the parallelism as 2D tiling. Flatten the work into a single index over `(seq_pairs x output_tiles)` and let OpenMP distribute it evenly:\n\n```c\nint pairs  = seq / 2;\nint otiles = out / 4;\nint ntiles = pairs * otiles;\n\n#pragma omp parallel for schedule(static) if(ntiles > 16)\nfor (int t = 0; t < ntiles; t++) {\n    int p  = t / otiles;\n    int ot = t % otiles;\n    // 2Sx4O micro-kernel for rows (p*2, p*2+1) and outputs (ot*4 .. ot*4+3)\n}\n```\n\nThe `if(ntiles > 16)` guard is important. OpenMP has overhead for thread creation and synchronization. For tiny inputs where the total work is less than 16 tiles, single-threaded execution is faster. This threshold was determined empirically.\n\nThe second place I added parallelism was the attention heads. The model has 12 attention heads that are completely independent. Each head operates on a 64-dimensional slice of Q, K, and V. The original implementation processed all 12 sequentially. Making them parallel was the single biggest speedup for long sequences.\n\n```c\n#pragma omp parallel for schedule(static) if(seq >= 4)\nfor (int h = 0; h < num_heads; h++) {\n    float *local_q  = amalloc(seq * head_dim * sizeof(float));\n    float *local_k  = amalloc(seq * head_dim * sizeof(float));\n    float *local_v  = amalloc(seq * head_dim * sizeof(float));\n    float *local_sc = amalloc(seq * seq * sizeof(float));\n\n    // gather, K-transpose, Q*K^T, softmax, scores*V, scatter\n\n    free(local_q);\n    free(local_k);\n    free(local_v);\n    free(local_sc);\n}\n```\n\nEach thread gets its own scratch buffers, allocated based on the actual sequence length. I initially tried sharing pre-allocated buffers, but that meant either serializing access or allocating for the maximum sequence length of 8192 tokens. At max length, the attention score matrix alone is `8192 * 8192 * 4 = 256MB` per head. Allocating 12 of those upfront would consume 3GB just for scratch space. Dynamic allocation based on actual sequence length keeps memory usage proportional to input size.\n\n\n\n## The K-Transpose Trick\n\nComputing Q times K-transpose is the attention bottleneck. The naive approach computes each score as a dot product between a query row and a key row. With AVX2, that means loading 8 floats from Q, 8 from K, multiplying, and then doing a horizontal sum to reduce 8 partial products to one scalar. The horizontal sum is the expensive part. It takes multiple shuffle and add instructions.\n\nI transpose K before the multiplication. Instead of K being stored as `[seq][head_dim]`, I rearrange it to `[head_dim][seq]`. Now computing one row of scores is a standard matrix-vector multiply: load 8 values from the transposed K column, multiply by the corresponding Q element (broadcast), and accumulate. No horizontal sum until the very end.\n\nThis changes the memory access pattern from scattered loads (one element per K row) to sequential loads (contiguous elements in the transposed layout). The CPU prefetcher handles sequential access much better than strided access. On a 100-token sequence, this optimization alone cut attention time by about 30%.\n\n\n\n## Memory Alignment\n\nEvery float buffer in the codebase is allocated with 32-byte alignment:\n\n```c\nstatic void *amalloc(size_t n)\n{\n    n = (n + 31) & ~(size_t)31;\n    if (n == 0) n = 32;\n    return aligned_alloc(32, n);\n}\n```\n\nThe rounding ensures the allocation size is always a multiple of 32, which is required by `aligned_alloc`. The zero check prevents undefined behavior (some implementations reject size 0).\n\nAligned memory lets me use `_mm256_load_ps` instead of `_mm256_loadu_ps`. On modern CPUs the difference is negligible because the hardware handles unaligned loads efficiently. But on some older microarchitectures, unaligned loads that cross a cache line boundary incur a penalty. Since alignment is free (just round up the allocation), there's no reason not to do it.\n\n\n\n## Benchmarks\n\nI spent almost as much time getting the benchmarks right as I did on the implementation. Unfair benchmarks are worse than no benchmarks. They mislead you into thinking your code is faster or slower than it actually is.\n\nThe main fairness concern was thread count. PyTorch uses Intel MKL by default, which detects the number of cores and spawns threads accordingly. My C implementation uses OpenMP, which does the same via `OMP_NUM_THREADS`. If you don't control both, you get meaningless numbers. I set `torch.set_num_threads` to match `OMP_NUM_THREADS` for every run.\n\nThe second concern was warmup. Both implementations have cold-start costs. PyTorch has JIT compilation. My code has cache warming. I run 5 warmup iterations before timing 20 measured iterations.\n\nThe third concern was measuring the right thing. The C implementation includes tokenization in its timing because `nomic_embed` takes a string and returns a float array. To be fair, I measured HuggingFace with tokenization included too. I also measured HuggingFace inference-only (pre-tokenized) separately, so you can see how much of the time is Python overhead versus actual compute.\n\nHere are the results at matched thread count (6 threads):\n\n| Input | Tokens | nomic.c (ms) | HuggingFace (ms) | Speedup |\n|-------|--------|-------------|---------------------|---------|\n| Short query | 8 | 30 | 64 | **2.1x** |\n| Medium query | 11 | 42 | 76 | **1.8x** |\n| Sentence | 15-19 | 51-52 | 77-89 | **1.5-1.7x** |\n| Short paragraph | 56 | 107 | 123 | **1.2x** |\n| Long paragraph | 101 | 137 | 173 | **1.3x** |\n| Full page | 211 | 255 | 303 | **1.2x** |\n\nThe pattern is clear. Short inputs show the biggest speedup because Python and PyTorch overhead is a larger fraction of the total time. For a short query, PyTorch spends more time in framework overhead than in actual matrix multiplication. The C implementation has essentially zero overhead. You call the function, it tokenizes, runs the model, and returns.\n\nAs inputs get longer, the actual compute dominates and the speedup converges to 1.2-1.3x. That remaining gap is the difference between hand-tuned AVX2 kernels and MKL's GEMM. MKL is extremely good, but it's also general-purpose. My kernels are specialized for the exact dimensions this model uses. I don't handle arbitrary matrix sizes, I don't support different data types, and I don't need to.\n\n\n\n## What I Learned\n\nWriting SIMD by hand is tedious but mechanical. Once you understand the instruction set, it's just a matter of mapping the scalar algorithm to vector operations. The Intel Intrinsics Guide is the only reference you need. The real difficulty is not the instructions themselves but the memory access patterns. A kernel that does fewer FLOPs but accesses memory sequentially will beat a kernel that does optimal arithmetic but touches memory randomly. Cache misses are the enemy, not instruction count.\n\nThe tokenizer was the most frustrating part. Unicode normalization has corner cases I never imagined. Combining characters, surrogate pairs, CJK ranges that span multiple Unicode blocks. Every edge case matters because tokens are the input to the model and there's no error correction downstream. If you get one token wrong, the embedding is wrong. I verified against HuggingFace's tokenizer on hundreds of inputs before I trusted it.\n\nOpenMP is underrated for this kind of workload. People reach for complex threading libraries when a simple `#pragma omp parallel for` on the right loop gives 80% of the theoretical speedup. The 2D tiling trick was important for keeping all cores busy, but the actual parallelism is just one line of code.\n\nThe model format should be as dumb as possible. I store weights as a flat binary file. The vocabulary goes first (30522 entries, length-prefixed strings), then every weight tensor concatenated in layer order. No metadata headers, no version numbers, no compression. The converter is 80 lines of Python that reads HuggingFace safetensors and writes bytes. Loading the model is a single `fread`. Simple formats are easy to debug, easy to verify, and fast to load.\n\n\n\n## The API\n\nThe whole thing is about 1100 lines of C in a single file. The public interface is four functions:\n\n```c\nnomic_ctx *nomic_load(const char *model_path);\nvoid       nomic_free(nomic_ctx *ctx);\nfloat     *nomic_embed(nomic_ctx *ctx, const char *text, int dim);\nfloat      nomic_similarity(const float *a, const float *b, int dim);\n```\n\nLoad a model. Embed text. Compare embeddings. Free the model. The `dim` parameter controls Matryoshka truncation. Pass 768 for full precision, 256 for 3x less storage with minimal quality loss, or 64 if you need maximum speed and can tolerate some accuracy degradation. All returned embeddings are L2-normalized regardless of dimension.\n\n```c\n#include \"nomic.h\"\n#include <stdio.h>\n#include <stdlib.h>\n\nint main(void) {\n    nomic_ctx *ctx = nomic_load(\"nomic.nomicmodel\");\n\n    float *a = nomic_embed(ctx, \"search_query: What is deep learning?\", 768);\n    float *b = nomic_embed(ctx, \"search_document: Deep learning uses neural networks.\", 768);\n    float *c = nomic_embed(ctx, \"search_document: The recipe calls for flour.\", 768);\n\n    printf(\"relevant:   %.4f\\n\", nomic_similarity(a, b, 768));\n    printf(\"irrelevant: %.4f\\n\", nomic_similarity(a, c, 768));\n\n    free(a); free(b); free(c);\n    nomic_free(ctx);\n}\n```\n\nCompile with `gcc -O2 -DUSE_AVX2 -mavx2 -mfma -fopenmp`, link `-lm`, and you have a self-contained embedding engine. No dependencies to install. No Python to configure. No Docker containers to pull. Just a C file, a header, and a model binary.\n\n55 tests cover the full pipeline from tokenizer edge cases to end-to-end embedding verification against the HuggingFace reference implementation."
		},
		"ja": {
			"title": "nomic.c",
			"date": "2026-01-27",
			"content": "[github.com/hyourindev/nomic.c](https://github.com/hyourindev/nomic.c)\n\n## 純C言語によるNomic Embed Text v1.5の推論\n\nテキスト埋め込みモデルをC言語でフルスクラッチ実装した。PyTorchもONNXもBLASも使っていない。C11とIntelのSIMD組み込み関数だけで、Nomic Embed Text v1.5を単一ファイルで動くように仕上げた。CPU上でHuggingFace Transformersと比較して1.2〜2.1倍の高速化を達成している。\n\n以下では、主要な設計判断の背景、効果のあった最適化、そしていくつかの失敗についてまとめる。\n\n\n\n## なぜ純Cなのか\n\n埋め込みモデルは、セマンティック検索、RAGパイプライン、クラスタリング、分類といった処理の中核を担っている。ベクトル検索に対応した検索バーにクエリを入力するたびに、どこかで埋め込みモデルが走っている。大抵はPyTorchだ。つまり、2GBのランタイム、Pythonインタプリタ、そして壊れないことを祈るしかない依存関係グラフが付いてくる。\n\n自分が求めていたのは別のものだった。任意のCプログラムから関数をひとつ呼び出し、float配列を受け取って終わり。ランタイムなし。三重の抽象化の裏に隠れたアロケータもなし。動的ディスパッチもなし。`#include \"nomic.h\"` して `-lm` でリンクするだけ。\n\nNomic Embed v1.5は良い対象だった。パラメータ数1.37億のBERTエンコーダで、重みはfloat32で522MBに収まる程度の規模だ。単純な実装では遅くなり、パフォーマンスについて真剣に考える必要がある、ちょうどよいサイズ感だ。さらにMatryoshka次元にも対応しており、埋め込みを64、128、256、512次元に切り詰めても有用な結果が得られる。実用的な機能なのでサポートする価値がある。\n\n\n\n## モデルの構造\n\nアーキテクチャはBERTだが、素のBERTではない。Nomicがいくつか変更を加えており、実装に影響する。\n\n学習済みの絶対位置埋め込みの代わりに、Rotary Position Embeddings（RoPE）を使用している。トークン埋め込みに位置ベクトルを加算する代わりに、次元のペアを位置に比例した角度で回転させる。回転周波数にはDynamic NTKスケーリングが適用されており、系列長が学習時の長さを超えた場合に基底周波数を調整する。したがって、静的テーブルの参照ではなく、実際の系列長に基づいてsin/cosテーブルを動的に計算する必要がある。\n\nフィードフォワードネットワークにはGELUの代わりにSwiGLUが使われている。SwiGLUは中間表現を二分割し、片方にSiLU（Sigmoid Linear Unit）を適用してから要素ごとに乗算する。このため、FFNの射影は768→3072が一度ではなく二度（ゲートと値）必要になる。パラメータもFLOPsも増えるが、パラメータあたりの品質は向上する。\n\nQKV射影は融合されている。Q、K、Vそれぞれに768×768の線形層を用意するのではなく、768×2304の単一の射影にまとめている。バイアス項はどこにもない。実装上の些細な点だが、コードは簡潔になる。\n\n残差結合はPost-norm方式を採用している。レイヤー正規化はAttentionやFFNブロックの前ではなく、残差加算の後に適用される。順伝播の順序がわずかに変わる。\n\n最終的な埋め込みパイプラインは、系列全体のmean pooling、最終レイヤー正規化、所望のMatryoshka次元への切り詰め、L2正規化の順で行われる。`nomic_embed`から出力される埋め込みはすべて単位ノルムであり、内積でそのままコサイン類似度を計算できる。\n\n\n\n## トークナイザ\n\nトークナイザがこのプロジェクトで最も苦労した部分だった。トークン化の概念が難しいからではない。BERTのトークン化が長年にわたって蓄積してきたエッジケースをすべて正確に再現しなければならないからだ。\n\nパイプラインはUnicode正規化から始まる。具体的にはNFD正規化で、アクセント付き文字を基底文字と結合記号に分解する。その後、結合記号を完全に除去する。つまり「café」は「cafe」になる。語彙がこの方法で構築されているため、トークナイザの挙動が一致しなければ異なるトークンIDが生成され、埋め込みがまったくの別物になる。\n\n正規化の後、CJK文字の前後に空白が挿入される。これはBERT特有の規則で、CJK統合漢字をそれぞれ独立した単語として扱う。その後すべて小文字に変換され、空白と句読点の境界で同時に分割される。\n\n分割された各単語に対してWordPiece分割が行われる。アルゴリズムは語彙中の最長一致接頭辞を探す。単語全体が一致すればそれで良し。一致しなければ、最長の一致接頭辞を出力し、残りに`##`を付けて処理を続ける。どの接頭辞も一致しなければ`[UNK]`を出力する。\n\n語彙は30,522エントリある。ソート済み配列に格納し、二分探索で検索している。最初はハッシュテーブルを試したが、この語彙サイズではキャッシュの局所性のおかげでソート済み配列のほうが速かった。各検索がメモリの連続領域にアクセスするのに対し、ハッシュテーブルはヒープ全体に散らばったアクセスになる。\n\nトークン化を完全に正しく動作させるのに、SIMDの実装全体より多くの時間を費やした。トークナイザ専用のテストを20件以上書き、CJKとラテン文字の混在テキスト、連続する句読点を含む文字列、多数のサブワードに分解される単語、空文字列などをカバーした。トークンがひとつでも違えば埋め込みは使い物にならない。緩やかな劣化など存在しない。\n\n\n\n## SIMD戦略\n\nモデル内の計算負荷の高い処理は、行列積か要素ごとのベクトル演算のいずれかだ。どちらもSIMDに自然に写像できる。\n\nターゲットはFMA付きAVX2とした。過去10年間に製造されたほぼすべてのx86-64 CPUで利用可能だ。`__m256`レジスタは8個のfloat32を保持する。モデル内のすべての次元（768, 2304, 3072）が8で割り切れるため、端数処理は一切不要。すべてのループが1回のイテレーションで正確に8個のfloatを処理し、余りもマスクもない。\n\nすべての処理の基盤となる3つのユーティリティ関数を紹介する。\n\n`hsum_avx`は`__m256`内の8つのfloatを単一のスカラに畳み込む。2回の`_mm256_hadd_ps`の後、上位・下位128ビットレーンを抽出して加算する。すべてのドット積がこの水平加算で終わるため、コードベース中で最も頻繁に使われる処理だ。\n\n`exp256_approx`は8個の値に対するベクトル化された`exp(x)`の近似を同時に計算する。アルゴリズムは`exp(x)`を`2^(x/ln2)`に分解し、整数部分（IEEE 754指数部のビットシフトになる）と小数部分（4次のHorner多項式で近似）に分ける。精度は約20ビット。softmaxやSiLUでは指数関数の比を計算するため、相対誤差が相殺され、20ビットで十分だ。\n\n`rcp_nr`は`_mm256_rcp_ps`（12ビット精度）を使って`1/x`を計算し、Newton-Raphson法の1ステップで精度を倍にする。SiLUのシグモイド計算で`_mm256_div_ps`を回避するために使う。除算は多くのマイクロアーキテクチャで乗算の3〜4倍遅い。\n\n`USE_AVX2`が定義されていない場合、SIMD層全体が完全に消える。すべてのカーネルに素のCループによるスカラフォールバックがある。つまり、ARM、RISC-V、その他あらゆるアーキテクチャでもビルド・実行が可能だ。ただし遅くなる。\n\n\n\n## GEMMマイクロカーネル\n\nコードベース全体で最も重要な関数は`linear_no_bias`だ。Transformerの全層にある全線形射影の行列積を担っている。Q, K, V射影、出力射影、FFNの拡大射影、FFNの縮小射影。12層で各6回の行列積。この関数が遅ければ、すべてが遅い。\n\n最初の実装は素朴な三重ループだった。正しく動いたが、致命的に遅い。\n\n二番目の実装では、出力ニューロンをひとつずつAVX2のドット積で処理した。改善はしたが、まだ性能を取りこぼしていた。ボトルネックはメモリ帯域幅だ。出力ニューロンごとに重み行列の1行を読み込み、1回だけ使って捨てる。演算密度が低すぎる。\n\n三番目の実装が最終版だ。2S×4Oのマイクロカーネルで、2つの系列位置と4つの出力ニューロンを同時に処理する。内側ループでは、4つの出力行からそれぞれ8個の重みをロードし、2つの系列行からそれぞれ8個の入力値をブロードキャストし、8つのアキュムレータレジスタに対して8回のFMA演算を実行する。\n\n```c\n__m256 a00 = _mm256_setzero_ps(), a01 = a00, a02 = a00, a03 = a00;\n__m256 a10 = _mm256_setzero_ps(), a11 = a10, a12 = a10, a13 = a10;\n\nfor (int k = 0; k < in; k += 8) {\n    __m256 x0 = _mm256_load_ps(row0 + k);\n    __m256 x1 = _mm256_load_ps(row1 + k);\n    __m256 w0 = _mm256_load_ps(W0 + k);\n    __m256 w1 = _mm256_load_ps(W1 + k);\n    __m256 w2 = _mm256_load_ps(W2 + k);\n    __m256 w3 = _mm256_load_ps(W3 + k);\n\n    a00 = _mm256_fmadd_ps(x0, w0, a00);\n    a01 = _mm256_fmadd_ps(x0, w1, a01);\n    a02 = _mm256_fmadd_ps(x0, w2, a02);\n    a03 = _mm256_fmadd_ps(x0, w3, a03);\n    a10 = _mm256_fmadd_ps(x1, w0, a10);\n    a11 = _mm256_fmadd_ps(x1, w1, a11);\n    a12 = _mm256_fmadd_ps(x1, w2, a12);\n    a13 = _mm256_fmadd_ps(x1, w3, a13);\n}\n```\n\nアキュムレータ8本、入力ロード2回、重みロード4回。16本中14本のYMMレジスタを使用。残り2本はFMAパイプラインの一時領域として使われる。レジスタ圧は高いが、ちょうど収まる。\n\n重要なのは、各重みロードが2つの系列行で共有される点だ。行をひとつずつ処理する場合と比べて、重みの帯域消費が半減する。演算密度が倍になり、カーネルはメモリ律速ではなく演算律速になる。\n\n重み行に対するソフトウェアプリフェッチも追加した。現在位置から16要素（64バイト＝1キャッシュライン）先を指定している。\n\n```c\n_mm_prefetch((const char *)(W0 + k + 16), _MM_HINT_T0);\n_mm_prefetch((const char *)(W1 + k + 16), _MM_HINT_T0);\n_mm_prefetch((const char *)(W2 + k + 16), _MM_HINT_T0);\n_mm_prefetch((const char *)(W3 + k + 16), _MM_HINT_T0);\n```\n\n次のキャッシュラインの読み込みを必要になる前に開始するようCPUに指示している。ハードウェアプリフェッチャが優秀な最近のCPUでは効果は限定的だが、コストはゼロであり、古いハードウェアでは効く。\n\n\n\n## 並列化\n\nシングルスレッド実装はシンプルだが、CPUの大部分を遊ばせてしまう。モデルにはデータ並列性が豊富にある。OpenMPを採用した。扱いが極めて簡単で、GCCがネイティブにサポートしている。\n\n最初に並列化を導入したのはGEMMカーネルだ。素朴なアプローチは系列位置で並列化することだが、系列が短いとコアが遊んでしまう。10トークンのクエリはCLSとSEPを加えても12×768の行列にしかならない。6ペアの行を6スレッドで並列化すると、各スレッドにちょうど1ペアずつ割り当てられる。負荷分散の余地がない。\n\nそこで2次元タイリングに再構成した。`(系列ペア数 × 出力タイル数)`の作業を1次元のインデックスに展開し、OpenMPに均等分配させる。\n\n```c\nint pairs  = seq / 2;\nint otiles = out / 4;\nint ntiles = pairs * otiles;\n\n#pragma omp parallel for schedule(static) if(ntiles > 16)\nfor (int t = 0; t < ntiles; t++) {\n    int p  = t / otiles;\n    int ot = t % otiles;\n    // 行 (p*2, p*2+1) と出力 (ot*4 .. ot*4+3) に対する2S×4Oマイクロカーネル\n}\n```\n\n`if(ntiles > 16)` のガード条件は重要だ。OpenMPにはスレッド生成と同期のオーバーヘッドがある。タイル総数が16未満の小さな入力では、シングルスレッド実行のほうが速い。この閾値は経験的に決定した。\n\n二番目に並列化したのはアテンションヘッドだ。12個のヘッドは完全に独立しており、それぞれQ, K, Vの64次元のスライスを処理する。元の実装は12ヘッドを逐次処理していた。これを並列化したのが、長い系列に対する最大の高速化となった。\n\n```c\n#pragma omp parallel for schedule(static) if(seq >= 4)\nfor (int h = 0; h < num_heads; h++) {\n    float *local_q  = amalloc(seq * head_dim * sizeof(float));\n    float *local_k  = amalloc(seq * head_dim * sizeof(float));\n    float *local_v  = amalloc(seq * head_dim * sizeof(float));\n    float *local_sc = amalloc(seq * seq * sizeof(float));\n\n    // gather, K転置, Q*K^T, softmax, scores*V, scatter\n\n    free(local_q);\n    free(local_k);\n    free(local_v);\n    free(local_sc);\n}\n```\n\n各スレッドは実際の系列長に基づいて確保された独自のスクラッチバッファを持つ。最初は事前確保したバッファを共有する方式を試したが、それではアクセスの直列化か、最大系列長8192トークン分の確保が必要になる。最大長では、アテンションスコア行列だけで1ヘッドあたり `8192 × 8192 × 4 = 256MB` になる。12ヘッド分を事前確保すると、スクラッチ領域だけで3GB消費する。実際の系列長に基づく動的確保なら、メモリ使用量が入力サイズに比例する。\n\n\n\n## K転置のテクニック\n\nQ×K転置の計算がアテンションのボトルネックだ。素朴な方法では、各スコアをクエリ行とキー行のドット積として計算する。AVX2では、Qから8個、Kから8個のfloatを読み込み、乗算し、水平加算で8つの部分積を1つのスカラに畳み込む。水平加算がコストの高い部分で、複数のシャッフルと加算命令を要する。\n\n乗算の前にKを転置する。Kを `[seq][head_dim]` ではなく `[head_dim][seq]` に並べ替える。すると、スコアの1行の計算は通常の行列ベクトル積になる。転置されたKの列から8個の値を読み込み、対応するQの要素をブロードキャストして乗算し、累積する。最後まで水平加算が不要になる。\n\nメモリアクセスパターンが、散発的なロード（K行ごとに1要素）から逐次的なロード（転置後のレイアウトで連続した要素）に変わる。CPUプリフェッチャは、ストライドアクセスよりシーケンシャルアクセスをはるかにうまく処理する。100トークンの系列では、この最適化だけでアテンション処理時間が約30%短縮された。\n\n\n\n## メモリアライメント\n\nコードベース内のすべてのfloatバッファは32バイト境界でアラインされている。\n\n```c\nstatic void *amalloc(size_t n)\n{\n    n = (n + 31) & ~(size_t)31;\n    if (n == 0) n = 32;\n    return aligned_alloc(32, n);\n}\n```\n\n丸め処理により、確保サイズは常に32の倍数になる。これは`aligned_alloc`の要件だ。ゼロチェックは未定義動作の回避のため（サイズ0を拒否する実装が存在する）。\n\nアライメントされたメモリにより、`_mm256_loadu_ps`の代わりに`_mm256_load_ps`が使える。最近のCPUではハードウェアが非アラインロードを効率的に処理するため差はほぼない。しかし、一部の古いマイクロアーキテクチャでは、キャッシュライン境界をまたぐ非アラインロードにペナルティが発生する。アライメントのコストは実質ゼロ（確保サイズを切り上げるだけ）なので、やらない理由がない。\n\n\n\n## ベンチマーク\n\nベンチマークの公正性確保には、実装そのものとほぼ同じだけの労力を費やした。不公平なベンチマークは、ベンチマークがないよりも悪い。コードが実際より速い、あるいは遅いという誤った認識を生む。\n\n公平性に関する最大の懸念はスレッド数だった。PyTorchはデフォルトでIntel MKLを使用し、コア数を検出してスレッドを生成する。C実装はOpenMPを使い、`OMP_NUM_THREADS`で同様のことを行う。両方を揃えなければ、意味のない数値になる。すべての実行で`torch.set_num_threads`を`OMP_NUM_THREADS`に合わせた。\n\n二番目の懸念はウォームアップだ。両方の実装にコールドスタートのコストがある。PyTorchにはJITコンパイル、C実装にはキャッシュの温まりがある。計測の前に5回のウォームアップイテレーションを実行し、20回の計測イテレーションの結果を採用した。\n\n三番目の懸念は、何を計測すべきかという点だ。C実装は`nomic_embed`が文字列を受け取ってfloat配列を返すため、トークン化も計測時間に含まれる。公平を期すため、HuggingFaceもトークン化込みで計測した。加えて、HuggingFaceの推論のみ（トークン化済み）の計測も別途行い、Python側のオーバーヘッドと実際の計算の割合が分かるようにした。\n\nスレッド数を揃えた結果（6スレッド）を示す。\n\n| 入力 | トークン数 | nomic.c (ms) | HuggingFace (ms) | 高速化率 |\n|------|-----------|-------------|---------------------|---------|\n| 短いクエリ | 8 | 30 | 64 | **2.1倍** |\n| 中程度のクエリ | 11 | 42 | 76 | **1.8倍** |\n| 文 | 15-19 | 51-52 | 77-89 | **1.5-1.7倍** |\n| 短い段落 | 56 | 107 | 123 | **1.2倍** |\n| 長い段落 | 101 | 137 | 173 | **1.3倍** |\n| 1ページ分 | 211 | 255 | 303 | **1.2倍** |\n\n傾向は明確だ。入力が短いほど高速化率が大きい。PythonとPyTorchのオーバーヘッドが全体の処理時間に占める割合が大きくなるためだ。短いクエリでは、PyTorchはフレームワークのオーバーヘッドに実際の行列積以上の時間を費やしている。C実装のオーバーヘッドは実質ゼロだ。関数を呼べば、トークン化してモデルを実行し、結果を返す。\n\n入力が長くなるにつれて実際の計算が支配的になり、高速化率は1.2〜1.3倍に収束する。この差は、手動チューニングしたAVX2カーネルとMKLのGEMMの差だ。MKLは極めて優秀だが、汎用的でもある。自分のカーネルはこのモデルが使う次元に特化している。任意の行列サイズに対応する必要も、異なるデータ型をサポートする必要もない。\n\n\n\n## 学んだこと\n\n手書きSIMDは地道だが機械的な作業だ。命令セットを理解すれば、あとはスカラアルゴリズムをベクトル演算に写像するだけだ。Intel Intrinsics Guideが唯一必要なリファレンスだ。真の難しさは命令そのものではなく、メモリアクセスパターンにある。FLOPs数が少なくてもメモリにシーケンシャルにアクセスするカーネルは、最適な演算をしていてもメモリにランダムアクセスするカーネルに勝る。敵は命令数ではなく、キャッシュミスだ。\n\nトークナイザが最もフラストレーションの溜まる部分だった。Unicode正規化には想像もしなかったコーナーケースがある。結合文字、サロゲートペア、複数のUnicodeブロックにまたがるCJK範囲。すべてのエッジケースが重要だ。トークンはモデルへの入力であり、下流にエラー訂正は存在しない。トークンが1つ違えば、埋め込みも間違う。信頼できるまでに、HuggingFaceのトークナイザとの照合を数百の入力で行った。\n\nOpenMPはこの種のワークロードに対して過小評価されている。複雑なスレッドライブラリに手を伸ばす人が多いが、適切なループに`#pragma omp parallel for`を一行入れるだけで理論上の速度向上の80%が得られる。2次元タイリングの工夫は全コアを稼働させるために重要だったが、並列化そのものはコード上たった一行だ。\n\nモデルフォーマットは可能な限り単純にすべきだ。重みはフラットなバイナリファイルとして格納している。語彙（30,522エントリ、長さプレフィックス付き文字列）が先頭にあり、その後にすべての重みテンソルがレイヤー順で連結される。メタデータヘッダも、バージョン番号も、圧縮もない。コンバータはHuggingFaceのsafetensorsを読み取ってバイト列を書き出す80行のPythonスクリプトだ。モデルの読み込みは`fread`一発。単純なフォーマットはデバッグしやすく、検証しやすく、読み込みが速い。\n\n\n\n## API\n\n全体で約1,100行のC言語コードを単一ファイルに収めている。公開インターフェースは4つの関数だ。\n\n```c\nnomic_ctx *nomic_load(const char *model_path);\nvoid       nomic_free(nomic_ctx *ctx);\nfloat     *nomic_embed(nomic_ctx *ctx, const char *text, int dim);\nfloat      nomic_similarity(const float *a, const float *b, int dim);\n```\n\nモデルを読み込む。テキストを埋め込む。埋め込みを比較する。モデルを解放する。`dim`パラメータでMatryoshkaの切り詰め次元を制御する。768でフル精度、256で容量3分の1かつ品質劣化は最小限、64で最大速度だが精度低下は許容する必要がある。返される埋め込みは次元に関わらずすべてL2正規化済みだ。\n\n```c\n#include \"nomic.h\"\n#include <stdio.h>\n#include <stdlib.h>\n\nint main(void) {\n    nomic_ctx *ctx = nomic_load(\"nomic.nomicmodel\");\n\n    float *a = nomic_embed(ctx, \"search_query: What is deep learning?\", 768);\n    float *b = nomic_embed(ctx, \"search_document: Deep learning uses neural networks.\", 768);\n    float *c = nomic_embed(ctx, \"search_document: The recipe calls for flour.\", 768);\n\n    printf(\"relevant:   %.4f\\n\", nomic_similarity(a, b, 768));\n    printf(\"irrelevant: %.4f\\n\", nomic_similarity(a, c, 768));\n\n    free(a); free(b); free(c);\n    nomic_free(ctx);\n}\n```\n\n`gcc -O2 -DUSE_AVX2 -mavx2 -mfma -fopenmp` でコンパイルし、`-lm` でリンクすれば、自己完結型の埋め込みエンジンが手に入る。インストールすべき依存関係はない。Pythonの設定も不要。Dockerコンテナを引っ張ってくる必要もない。Cファイルとヘッダとモデルバイナリだけだ。\n\n55件のテストが、トークナイザのエッジケースからHuggingFaceリファレンス実装との埋め込み一致検証まで、パイプライン全体をカバーしている。"
		}
	}
}